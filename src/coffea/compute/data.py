from abc import ABC, abstractmethod
from collections.abc import Callable, Iterator
from copy import deepcopy
from dataclasses import dataclass
from functools import partial
from itertools import chain, repeat
from typing import Generic, Literal, TypeVar

import uproot
from more_itertools import roundrobin
from uproot import ReadOnlyDirectory

from coffea.compute.func import EventsArray, Processor
from coffea.compute.protocol import DataElement, InputT, ResultT


@dataclass(frozen=True)
class DataWorkElement(Generic[InputT, ResultT]):
    """Concrete WorkElement, applies func to loaded item.

    TODO: do we really need a protocol and a concrete class for this or can it just be the concrete class?
    """

    func: Callable[[InputT], ResultT]
    item: DataElement[InputT]

    def __call__(self) -> ResultT:
        return self.func(self.item.load())


@dataclass(frozen=True)
class MapData(Generic[InputT, ResultT]):
    """Concrete Computable, generates DataWorkElements by calling iter_gen and applying func."""

    func: Callable[[InputT], ResultT]
    "Function to apply to each loaded DataElement."
    iter_gen: Callable[[], Iterator[DataElement[InputT]]]
    "Iterator generator over DataElements. Should be callable to allow re-iteration."

    def __iter__(self) -> Iterator[DataWorkElement[InputT, ResultT]]:
        return map(DataWorkElement, repeat(self.func), self.iter_gen())


StepContextT = TypeVar("StepContextT", bound="ContextFile")
"""A context for the current chunk of events being processed.

This can be used to pass along metadata about the chunk, such as
which file it came from, which dataset that file is part of, etc.
"""


@dataclass(frozen=True)
class Chunk(Generic[StepContextT]):
    events: EventsArray
    context: StepContextT


@dataclass(frozen=True)
class StepElement(Generic[StepContextT]):
    """A step element represents a chunk of events to be processed from a file.

    It is generated by any StepIterable implementation. It is supposed to live the duration
    of a task, but not serialized long-term.
    """

    entry_range: tuple[int, int]
    context: StepContextT

    def __len__(self) -> int:
        return self.entry_range[1] - self.entry_range[0]

    def load(self) -> Chunk[StepContextT]:
        # TODO: implementation of event loading
        info = f"{self.entry_range},{self.context.file_path}"
        events = info + "A" * (len(self) - len(info))
        return Chunk(events=events, context=self.context)


def _func_chunk(func: Processor[ResultT], chunk: Chunk[StepContextT]) -> ResultT:
    """Toss away the context and just call the processor on the events.

    TODO: this is until we define how Processors handle context properly.
    """

    return func.process(chunk.events)


class StepIterable(ABC, Generic[StepContextT]):
    @abstractmethod
    def iter_steps(self) -> Iterator[StepElement[StepContextT]]:
        """Return an iterator over steps in the computation."""
        raise NotImplementedError

    def map_steps(
        self, func: Callable[[Chunk[StepContextT]], ResultT] | Processor[ResultT]
    ):
        """Apply a function or Processor to each step in this data."""
        if not callable(func):
            func = partial(_func_chunk, func)
        return MapData(func=func, iter_gen=self.iter_steps)


FileContextT = TypeVar("FileContextT", bound="ContextDataset")
"""A context for the current file being processed."""


@dataclass(frozen=True)
class OpenFile(Generic[FileContextT]):
    root_dir: ReadOnlyDirectory
    file_path: str
    context: FileContextT


@dataclass(frozen=True)
class FileElement(Generic[FileContextT]):
    """A FileElement is a lightweight representation of a file to be loaded.

    It is generated by any FileIterable implementation. It is supposed to live the duration
    of a task, but not serialized long-term. For long-term storage, use File.
    """

    path: str
    context: FileContextT

    def load(self) -> OpenFile[FileContextT]:
        file = uproot.open(self.path)
        # TODO: find a way to keep this while mocking in tests
        # assert isinstance(file, ReadOnlyDirectory)
        return OpenFile(file, self.path, self.context)


class FileIterable(ABC, Generic[FileContextT]):
    @abstractmethod
    def iter_files(self) -> Iterator[FileElement[FileContextT]]:
        """Return an iterator over files in the computation."""
        raise NotImplementedError

    def map_files(self, func: Callable[[OpenFile[FileContextT]], ResultT]):
        return MapData(func=func, iter_gen=self.iter_files)


@dataclass
class ContextFile:
    """Based on the File properties we'd like to pass as context to each step."""

    file_path: str
    uuid: str


@dataclass
class File(StepIterable[ContextFile]):
    path: str
    steps: list[tuple[int, int]]
    # TODO: object path within the file
    uuid: str = ""

    def iter_steps(self) -> Iterator[StepElement[ContextFile]]:
        context = ContextFile(file_path=self.path, uuid=self.uuid)
        return map(StepElement, self.steps, repeat(context))


@dataclass
class ContextDataset:
    dataset_name: str
    """Name of the dataset."""
    cross_section: float
    """Cross section in pb."""


# Note: field order is from right to left when inheriting from multiple dataclasses


@dataclass
class StepContextDataset(ContextDataset, ContextFile):
    """Metadata associated with a dataset."""

    @classmethod
    def wrap_file_step(
        cls,
        file_step: StepElement[ContextFile],
        context: ContextDataset,
    ) -> "StepElement[StepContextDataset]":
        # TODO: is the fact we use frozen dataclass and have to recreate a performance issue?
        return StepElement(
            entry_range=file_step.entry_range,
            context=cls(
                file_path=file_step.context.file_path,
                uuid=file_step.context.uuid,
                **context.__dict__,
            ),
        )


@dataclass
class Dataset(StepIterable[StepContextDataset], FileIterable[ContextDataset]):
    files: list[File]
    name: str
    traversal: Literal["depth", "breadth"] = "depth"
    """The traversal strategy for iterating over files in the dataset.

    "depth" means to process all steps in one file before moving to the next file.
    "breadth" means to process the first step in all files, then the second step in all files, etc.
    """

    def iter_steps(self) -> Iterator[StepElement[StepContextDataset]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(File.iter_steps, self.files))
        else:
            iterable = chain.from_iterable(map(File.iter_steps, self.files))
        contextwrap = partial(
            StepContextDataset.wrap_file_step,
            context=ContextDataset(
                dataset_name=self.name,
                cross_section=1.0,  # TODO: placeholder
            ),
        )
        return map(contextwrap, iterable)

    def iter_files(self) -> Iterator[FileElement[ContextDataset]]:
        return map(
            FileElement,
            (f.path for f in self.files),
            repeat(ContextDataset(dataset_name=self.name, cross_section=1.0)),
        )

    def __add__(self, other: "Dataset") -> "Dataset":
        assert self.name == other.name
        return Dataset(
            files=self.files + other.files,
            name=self.name,
            traversal=self.traversal,
        )


@dataclass
class ContextDataGroup:
    """Metadata associated with a data group."""

    group_name: str
    """Name of the data group."""


@dataclass
class StepContextDataGroup(ContextDataGroup, StepContextDataset):
    @classmethod
    def wrap_dataset_step(
        cls,
        dataset_step: StepElement[StepContextDataset],
        context: ContextDataGroup,
    ) -> "StepElement[StepContextDataGroup]":
        return StepElement(
            entry_range=dataset_step.entry_range,
            context=cls(
                **dataset_step.context.__dict__,
                **context.__dict__,
            ),
        )


@dataclass
class FileContextDataGroup(ContextDataGroup, ContextDataset):
    """Metadata associated with a data group file."""

    @classmethod
    def wrap_dataset_file(
        cls,
        dataset_file: FileElement[ContextDataset],
        context: ContextDataGroup,
    ) -> "FileElement[FileContextDataGroup]":
        return FileElement(
            path=dataset_file.path,
            context=cls(
                **dataset_file.context.__dict__,
                **context.__dict__,
            ),
        )


@dataclass
class DataGroup(StepIterable[StepContextDataGroup], FileIterable[FileContextDataGroup]):
    datasets: list[Dataset]
    name: str
    traversal: Literal["depth", "breadth"] = "depth"
    """The traversal strategy for iterating over datasets in the group."""

    def iter_steps(self) -> Iterator[StepElement[StepContextDataGroup]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(Dataset.iter_steps, self.datasets))
        else:
            iterable = chain.from_iterable(map(Dataset.iter_steps, self.datasets))
        contextwrap = partial(
            StepContextDataGroup.wrap_dataset_step,
            context=ContextDataGroup(group_name=self.name),
        )
        return map(contextwrap, iterable)

    def iter_files(self) -> Iterator[FileElement[FileContextDataGroup]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(Dataset.iter_files, self.datasets))
        else:
            iterable = chain.from_iterable(map(Dataset.iter_files, self.datasets))
        contextwrap = partial(
            FileContextDataGroup.wrap_dataset_file,
            context=ContextDataGroup(group_name=self.name),
        )
        return map(contextwrap, iterable)

    def __add__(self, other: "DataGroup") -> "DataGroup":
        assert self.name == other.name
        datasets = deepcopy(self.datasets)
        dsdict = {ds.name: ds for ds in datasets}
        for od in other.datasets:
            if od.name in dsdict:
                dsdict[od.name] = dsdict[od.name] + od
            else:
                datasets.append(od)
        return DataGroup(datasets=datasets, name=self.name, traversal=self.traversal)


@dataclass
class InputDataset(FileIterable[ContextDataset]):
    files: list[str]
    name: str

    def iter_files(self) -> Iterator[FileElement[ContextDataset]]:
        return map(
            FileElement,
            self.files,
            repeat(ContextDataset(dataset_name=self.name, cross_section=1.0)),
        )


@dataclass
class InputDataGroup(FileIterable[FileContextDataGroup]):
    datasets: list[InputDataset]
    name: str

    def iter_files(self) -> Iterator[FileElement[FileContextDataGroup]]:
        iterable = chain.from_iterable(map(InputDataset.iter_files, self.datasets))
        contextwrap = partial(
            FileContextDataGroup.wrap_dataset_file,
            context=ContextDataGroup(group_name=self.name),
        )
        return map(contextwrap, iterable)
