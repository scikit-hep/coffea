from abc import ABC, abstractmethod
from collections.abc import Callable, Iterator
from dataclasses import dataclass
from functools import partial
from itertools import chain, repeat
from typing import Generic, Literal, TypeAlias

import uproot
from more_itertools import roundrobin
from uproot import ReadOnlyDirectory

from coffea.compute.context import (
    Context,
    ContextDataElement,
    ContextInput,
    Ctx_co,
    update_context,
    with_context,
)
from coffea.compute.func import EventsArray, Processor
from coffea.compute.group import GroupedFunction
from coffea.compute.protocol import DataElement, InputT, ResultT


@dataclass(frozen=True)
class DataWorkElement(Generic[InputT, ResultT]):
    """Concrete WorkElement, applies func to loaded item.

    TODO: do we really need a protocol and a concrete class for this or can it just be the concrete class?
    """

    func: Callable[[InputT], ResultT]
    item: DataElement[InputT]

    def __call__(self) -> ResultT:
        return self.func(self.item.load())


@dataclass(frozen=True)
class MapData(Generic[InputT, ResultT]):
    """Concrete Computable, generates DataWorkElements by calling iter_gen and applying func."""

    func: Callable[[InputT], ResultT]
    "Function to apply to each loaded DataElement."
    make_iter: Callable[[], Iterator[DataElement[InputT]]]
    "Callable to make an iterator over DataElements. Must be pure to allow re-iteration."

    def __iter__(self) -> Iterator[DataWorkElement[InputT, ResultT]]:
        return map(DataWorkElement, repeat(self.func), self.make_iter())


Chunk: TypeAlias = ContextInput[EventsArray, Ctx_co]
ChunkElement: TypeAlias = ContextDataElement[EventsArray, Ctx_co]


@dataclass(frozen=True)
class StepElement:
    """A step element represents a chunk of events to be processed from a file.

    It is generated by any StepIterable implementation. It is supposed to live the duration
    of a task, but not serialized long-term.
    """

    entry_range: tuple[int, int]
    file_path: str

    def __len__(self) -> int:
        return self.entry_range[1] - self.entry_range[0]

    def load(self) -> EventsArray:
        # TODO: implementation of event loading
        info = f"{self.entry_range},{self.file_path}"
        events = info + "A" * (len(self) - len(info))
        return events


def _func_chunk(func: Processor[ResultT], chunk: Chunk[Ctx_co]) -> ResultT:
    """Toss away the context and just call the processor on the events.

    TODO: this is until we define how Processors handle context properly.
    """

    return func.process(chunk.data)


class StepIterable(ABC, Generic[Ctx_co]):
    @abstractmethod
    def iter_steps(self) -> Iterator[ChunkElement[Ctx_co]]:
        """Return an iterator over steps in the computation."""
        raise NotImplementedError

    def map_steps(self, func: Callable[[Chunk[Ctx_co]], ResultT] | Processor[ResultT]):
        """Apply a function or Processor to each step in this data."""
        if not callable(func):
            func = partial(_func_chunk, func)
        return MapData(func=func, make_iter=self.iter_steps)


@dataclass(frozen=True)
class OpenFile:
    root_dir: ReadOnlyDirectory
    file_path: str


@dataclass(frozen=True)
class FileElement:
    """A FileElement is a lightweight representation of a file to be loaded.

    It is generated by any FileIterable implementation. It is supposed to live the duration
    of a task, but not serialized long-term. For long-term storage, use File.
    """

    path: str

    def load(self) -> OpenFile:
        file = uproot.open(self.path)
        # TODO: find a way to keep this while mocking in tests
        # assert isinstance(file, ReadOnlyDirectory)
        return OpenFile(file, self.path)


class FileIterable(ABC, Generic[Ctx_co]):
    @abstractmethod
    def iter_files(self) -> Iterator[ContextDataElement[OpenFile, Ctx_co]]:
        """Return an iterator over files in the computation."""
        raise NotImplementedError

    def map_files(self, func: Callable[[ContextInput[OpenFile, Ctx_co]], ResultT]):
        """Apply a function to each file in this data."""
        return MapData(func=func, make_iter=self.iter_files)

    def map_files_by(
        self,
        func: Callable[[ContextInput[OpenFile, Ctx_co]], ResultT],
        grouper: Callable[[Ctx_co], str],
    ):
        """Apply a function to each file in this data, grouping the output according to the key function."""
        grouped = GroupedFunction(func=func, key_func=grouper)
        return MapData(func=grouped, make_iter=self.iter_files)


@dataclass(frozen=True)
class ContextFile(Context):
    """Based on the File properties we'd like to pass as context to each step."""

    file_path: str
    uuid: str


@dataclass
class File(StepIterable[ContextFile]):
    path: str
    steps: list[tuple[int, int]]
    # TODO: object path within the file
    uuid: str = ""

    def iter_steps(self) -> Iterator[ChunkElement[ContextFile]]:
        steps = map(StepElement, self.steps, repeat(self.path))
        context = ContextFile(file_path=self.path, uuid=self.uuid)
        return with_context(steps, context)


@dataclass(frozen=True)
class ContextDataset(Context):
    dataset_name: str
    """Name of the dataset."""
    cross_section: float | None
    """Cross section in pb."""


# Note: field order is from right to left when inheriting from multiple dataclasses


@dataclass(frozen=True)
class StepContextDataset(ContextDataset, ContextFile):
    """Metadata associated with a dataset."""

    @classmethod
    def update_from_file(
        cls,
        mixin: ContextDataset,
        base: ContextFile,
    ) -> "StepContextDataset":
        return StepContextDataset(**base.__dict__, **mixin.__dict__)


@dataclass
class Dataset(StepIterable[StepContextDataset], FileIterable[ContextDataset]):
    files: list[File]
    metadata: ContextDataset
    traversal: Literal["depth", "breadth"] = "depth"
    """The traversal strategy for iterating over files in the dataset.

    "depth" means to process all steps in one file before moving to the next file.
    "breadth" means to process the first step in all files, then the second step in all files, etc.
    """

    def iter_steps(self) -> Iterator[ChunkElement[StepContextDataset]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(File.iter_steps, self.files))
        else:
            iterable = chain.from_iterable(map(File.iter_steps, self.files))
        return update_context(
            iterable, partial(StepContextDataset.update_from_file, self.metadata)
        )

    def iter_files(self) -> Iterator[ContextDataElement[OpenFile, ContextDataset]]:
        files = map(FileElement, (f.path for f in self.files))
        return with_context(files, self.metadata)


@dataclass(frozen=True)
class ContextDataGroup:
    """Metadata associated with a data group."""

    group_name: str
    """Name of the data group."""


@dataclass(frozen=True)
class StepContextDataGroup(ContextDataGroup, StepContextDataset):
    @classmethod
    def update_from_dataset(
        cls,
        mixin: ContextDataGroup,
        base: StepContextDataset,
    ) -> "StepContextDataGroup":
        return StepContextDataGroup(**base.__dict__, **mixin.__dict__)


@dataclass(frozen=True)
class FileContextDataGroup(ContextDataGroup, ContextDataset):
    """Metadata associated with a data group file."""

    @classmethod
    def update_from_dataset(
        cls,
        mixin: ContextDataGroup,
        base: ContextDataset,
    ) -> "FileContextDataGroup":
        return FileContextDataGroup(**base.__dict__, **mixin.__dict__)


@dataclass
class DataGroup(StepIterable[StepContextDataGroup], FileIterable[FileContextDataGroup]):
    datasets: list[Dataset]
    metadata: ContextDataGroup
    traversal: Literal["depth", "breadth"] = "depth"
    """The traversal strategy for iterating over datasets in the group."""

    def iter_steps(self) -> Iterator[ChunkElement[StepContextDataGroup]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(Dataset.iter_steps, self.datasets))
        else:
            iterable = chain.from_iterable(map(Dataset.iter_steps, self.datasets))
        return update_context(
            iterable,
            partial(
                StepContextDataGroup.update_from_dataset,
                self.metadata,
            ),
        )

    def iter_files(
        self,
    ) -> Iterator[ContextDataElement[OpenFile, FileContextDataGroup]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(Dataset.iter_files, self.datasets))
        else:
            iterable = chain.from_iterable(map(Dataset.iter_files, self.datasets))
        return update_context(
            iterable,
            partial(
                FileContextDataGroup.update_from_dataset,
                self.metadata,
            ),
        )


@dataclass
class InputDataset(FileIterable[ContextDataset]):
    files: list[str]
    metadata: ContextDataset

    def iter_files(self) -> Iterator[ContextDataElement[OpenFile, ContextDataset]]:
        files = map(FileElement, self.files)
        return with_context(files, self.metadata)


@dataclass
class InputDataGroup(FileIterable[FileContextDataGroup]):
    datasets: list[InputDataset]
    metadata: ContextDataGroup

    def iter_files(
        self,
    ) -> Iterator[ContextDataElement[OpenFile, FileContextDataGroup]]:
        iterable = chain.from_iterable(map(InputDataset.iter_files, self.datasets))
        return update_context(
            iterable,
            partial(
                FileContextDataGroup.update_from_dataset,
                self.metadata,
            ),
        )
