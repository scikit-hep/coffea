from abc import ABC, abstractmethod
from collections.abc import Callable, Iterator
from copy import deepcopy
from dataclasses import dataclass
from functools import partial
from itertools import chain, repeat
from typing import Generic, Literal, TypeVar

import uproot
from more_itertools import roundrobin
from uproot import ReadOnlyDirectory

from coffea.compute.func import EventsArray, Processor
from coffea.compute.protocol import ResultT

StepContextT = TypeVar("StepContextT", bound="StepContextFile")
"""A context for the current chunk of events being processed.

This can be used to pass along metadata about the chunk, such as
which file it came from, which dataset that file is part of, etc.
"""


@dataclass(frozen=True)
class Chunk(Generic[StepContextT]):
    events: EventsArray
    context: StepContextT


@dataclass(frozen=True)
class StepElement(Generic[StepContextT]):
    """A step element represents a chunk of events to be processed from a file.

    It is generated by any StepIterable implementation. It is supposed to live the duration
    of a task, but not serialized long-term.
    """

    entry_range: tuple[int, int]
    context: StepContextT

    def __len__(self) -> int:
        return self.entry_range[1] - self.entry_range[0]

    def load(self) -> Chunk[StepContextT]:
        # TODO: implementation of event loading
        info = f"{self.entry_range},{self.context.file_path}"
        events = info + "A" * (len(self) - len(info))
        return Chunk(events=events, context=self.context)


def _func_chunk(func: Processor[ResultT], chunk: Chunk[StepContextT]) -> ResultT:
    """Toss away the context and just call the processor on the events.

    TODO: this is until we define how Processors handle context properly.
    """

    return func.process(chunk.events)


class StepIterable(ABC, Generic[StepContextT]):
    @abstractmethod
    def iter_steps(self) -> Iterator[StepElement[StepContextT]]:
        """Return an iterator over steps in the computation."""
        raise NotImplementedError

    def map_steps(
        self, func: Callable[[Chunk[StepContextT]], ResultT] | Processor[ResultT]
    ) -> "StepwiseComputable[StepContextT, ResultT]":
        """Apply a function or Processor to each step in this data."""
        if not callable(func):
            func = partial(_func_chunk, func)
        return StepwiseComputable(func=func, iterable=self)


@dataclass(frozen=True)
class StepWorkElement(Generic[StepContextT, ResultT]):
    func: Callable[[Chunk[StepContextT]], ResultT]
    item: StepElement[StepContextT]

    def __call__(self) -> ResultT:
        return self.func(self.item.load())


@dataclass(frozen=True)
class StepwiseComputable(Generic[StepContextT, ResultT]):
    func: Callable[[Chunk[StepContextT]], ResultT]
    iterable: StepIterable[StepContextT]

    def __iter__(self) -> Iterator[StepWorkElement[StepContextT, ResultT]]:
        return map(StepWorkElement, repeat(self.func), self.iterable.iter_steps())


FileContextT = TypeVar("FileContextT", bound="FileContextDataset")
"""A context for the current file being processed."""


@dataclass(frozen=True)
class OpenFile(Generic[FileContextT]):
    root_dir: ReadOnlyDirectory
    file_path: str
    context: FileContextT


@dataclass(frozen=True)
class FileElement(Generic[FileContextT]):
    """A FileElement is a lightweight representation of a file to be loaded.

    It is generated by any FileIterable implementation. It is supposed to live the duration
    of a task, but not serialized long-term. For long-term storage, use File.
    """

    path: str
    context: FileContextT

    def load(self) -> OpenFile:
        file = uproot.open(self.path)
        # assert isinstance(file, ReadOnlyDirectory)
        return OpenFile(file, self.path, self.context)


class FileIterable(ABC, Generic[FileContextT]):
    @abstractmethod
    def iter_files(self) -> Iterator[FileElement[FileContextT]]:
        """Return an iterator over files in the computation."""
        raise NotImplementedError

    def map_files(
        self, func: Callable[[OpenFile[FileContextT]], ResultT]
    ) -> "FilewiseComputable[FileContextT, ResultT]":
        return FilewiseComputable(func=func, iterable=self)


@dataclass(frozen=True)
class FileWorkElement(Generic[FileContextT, ResultT]):
    func: Callable[[OpenFile[FileContextT]], ResultT]
    item: FileElement[FileContextT]

    def __call__(self) -> ResultT:
        return self.func(self.item.load())


@dataclass(frozen=True)
class FilewiseComputable(Generic[FileContextT, ResultT]):
    func: Callable[[OpenFile[FileContextT]], ResultT]
    iterable: FileIterable[FileContextT]

    def __iter__(self) -> Iterator[FileWorkElement[FileContextT, ResultT]]:
        return map(FileWorkElement, repeat(self.func), self.iterable.iter_files())


@dataclass
class StepContextFile:
    """Based on the File properties we'd like to pass as context to each step."""

    file_path: str
    uuid: str


@dataclass
class File(StepIterable[StepContextFile]):
    path: str
    steps: list[tuple[int, int]]
    # TODO: object path within the file
    uuid: str = ""

    def iter_steps(self) -> Iterator[StepElement[StepContextFile]]:
        for step in self.steps:
            yield StepElement(
                entry_range=step,
                context=StepContextFile(file_path=self.path, uuid=self.uuid),
            )
        # context = StepContextFile(file_path=self.path, uuid=self.uuid)
        # return map(StepElement, repeat(self.path), self.steps, repeat(context))


@dataclass
class StepContextDataset(StepContextFile):
    """Metadata associated with a dataset."""

    dataset_name: str
    """Name of the dataset."""
    cross_section: float
    """Cross section in pb."""

    @classmethod
    def wrap_file_step(
        cls,
        file_step: StepElement[StepContextFile],
        dataset_name: str,
        cross_section: float,
    ) -> "StepElement[StepContextDataset]":
        # TODO: is the fact we use frozen dataclass and have to recreate a performance issue?
        return StepElement(
            entry_range=file_step.entry_range,
            context=cls(
                file_path=file_step.context.file_path,
                uuid=file_step.context.uuid,
                dataset_name=dataset_name,
                cross_section=cross_section,
            ),
        )


@dataclass
class FileContextDataset:
    """Metadata associated with a dataset file."""

    dataset_name: str
    """Name of the dataset."""
    cross_section: float
    """Cross section in pb."""


@dataclass
class Dataset(StepIterable[StepContextDataset], FileIterable[FileContextDataset]):
    files: list[File]
    name: str
    traversal: Literal["depth", "breadth"] = "depth"
    """The traversal strategy for iterating over files in the dataset.

    "depth" means to process all steps in one file before moving to the next file.
    "breadth" means to process the first step in all files, then the second step in all files, etc.
    """

    def iter_steps(self) -> Iterator[StepElement[StepContextDataset]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(File.iter_steps, self.files))
        else:
            iterable = chain.from_iterable(map(File.iter_steps, self.files))
        contextwrap = partial(
            StepContextDataset.wrap_file_step,
            dataset_name=self.name,
            cross_section=1.0,  # TODO: placeholder
        )
        return map(contextwrap, iterable)

    def iter_files(self) -> Iterator[FileElement[FileContextDataset]]:
        return map(
            FileElement,
            (f.path for f in self.files),
            repeat(FileContextDataset(dataset_name=self.name, cross_section=1.0)),
        )

    def __add__(self, other: "Dataset") -> "Dataset":
        assert self.name == other.name
        return Dataset(
            files=self.files + other.files,
            name=self.name,
            traversal=self.traversal,
        )


@dataclass
class StepContextDataGroup(StepContextDataset):
    """Metadata associated with a data group."""

    group_name: str
    """Name of the data group."""

    @classmethod
    def wrap_dataset_step(
        cls,
        dataset_step: StepElement[StepContextDataset],
        group_name: str,
    ) -> "StepElement[StepContextDataGroup]":
        return StepElement(
            entry_range=dataset_step.entry_range,
            context=cls(
                file_path=dataset_step.context.file_path,
                uuid=dataset_step.context.uuid,
                dataset_name=dataset_step.context.dataset_name,
                cross_section=dataset_step.context.cross_section,
                group_name=group_name,
            ),
        )


@dataclass
class FileContextDataGroup(FileContextDataset):
    """Metadata associated with a data group file."""

    group_name: str
    """Name of the data group."""

    @classmethod
    def wrap_dataset_file(
        cls,
        dataset_file: FileElement[FileContextDataset],
        group_name: str,
    ) -> "FileElement[FileContextDataGroup]":
        return FileElement(
            path=dataset_file.path,
            context=cls(
                dataset_name=dataset_file.context.dataset_name,
                cross_section=dataset_file.context.cross_section,
                group_name=group_name,
            ),
        )


@dataclass
class DataGroup(StepIterable[StepContextDataGroup], FileIterable[FileContextDataGroup]):
    datasets: list[Dataset]
    name: str
    traversal: Literal["depth", "breadth"] = "depth"
    """The traversal strategy for iterating over datasets in the group."""

    def iter_steps(self) -> Iterator[StepElement[StepContextDataGroup]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(Dataset.iter_steps, self.datasets))
        else:
            iterable = chain.from_iterable(map(Dataset.iter_steps, self.datasets))
        contextwrap = partial(
            StepContextDataGroup.wrap_dataset_step,
            group_name=self.name,
        )
        return map(contextwrap, iterable)

    def iter_files(self) -> Iterator[FileElement[FileContextDataGroup]]:
        if self.traversal == "breadth":
            iterable = roundrobin(*map(Dataset.iter_files, self.datasets))
        else:
            iterable = chain.from_iterable(map(Dataset.iter_files, self.datasets))
        contextwrap = partial(
            FileContextDataGroup.wrap_dataset_file,
            group_name=self.name,
        )
        return map(contextwrap, iterable)

    def __add__(self, other: "DataGroup") -> "DataGroup":
        assert self.name == other.name
        datasets = deepcopy(self.datasets)
        dsdict = {ds.name: ds for ds in datasets}
        for od in other.datasets:
            if od.name in dsdict:
                dsdict[od.name] = dsdict[od.name] + od
            else:
                datasets.append(od)
        return DataGroup(datasets=datasets, name=self.name, traversal=self.traversal)


@dataclass
class InputDataset(FileIterable[FileContextDataset]):
    files: list[str]
    name: str

    def iter_files(self) -> Iterator[FileElement[FileContextDataset]]:
        return map(
            FileElement,
            self.files,
            repeat(FileContextDataset(dataset_name=self.name, cross_section=1.0)),
        )


@dataclass
class InputDataGroup(FileIterable[FileContextDataGroup]):
    datasets: list[InputDataset]
    name: str

    def iter_files(self) -> Iterator[FileElement[FileContextDataGroup]]:
        iterable = chain.from_iterable(map(InputDataset.iter_files, self.datasets))
        contextwrap = partial(
            FileContextDataGroup.wrap_dataset_file,
            group_name=self.name,
        )
        return map(contextwrap, iterable)
